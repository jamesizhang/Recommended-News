# 新闻推荐系统 - 简历项目描述（简洁版）

## 项目名称
基于深度学习的新闻推荐系统

## 项目描述（适合简历的简洁版本）

**项目时间**: [填写时间]  
**项目角色**: [开发者/负责人]  
**技术栈**: Python, PyTorch, TensorFlow, LightGBM, FAISS

### 项目简介
基于天池新闻推荐赛数据集，设计并实现了一套完整的新闻推荐系统。**核心创新是将Transformer架构应用于推荐系统，实现生成式召回和排序**。通过next-token prediction方式训练，Transformer模型既可以用于召回阶段（TopK检索），也可以直接用于排序预测，实现端到端的推荐。系统还集成了ItemCF、Embedding、Word2Vec、YouTubeDNN等传统方法作为补充，处理30万用户、近300万次点击、36万篇新闻文章的大规模数据。

### 主要工作
1. **Transformer生成式召回/排序（核心创新）**：
   - 基于Transformer Encoder架构，使用自注意力机制捕获用户行为序列的长期依赖关系
   - 通过next-token prediction方式训练，将推荐问题转化为序列生成问题
   - 支持用户ID embedding、用户特征（设备、地区等）和物品特征（字数、embedding等）的端到端融合
   - 模型输出物品logits分数，既可以用于召回（TopK检索），也可以直接用于排序预测
   - 实现一个模型同时完成召回和排序，简化推荐系统架构

2. **多路召回策略**：实现了ItemCF、Embedding相似度、Word2Vec、YouTubeDNN等4种传统召回方法，与Transformer一起通过加权融合提升召回覆盖率和多样性

3. **特征工程**：构建了24维特征体系，包括用户行为统计、物品属性、交互特征等。对DIN模型的dense特征和Transformer的物品特征使用MinMaxScaler进行归一化处理

4. **排序模型**：实现了Transformer直接排序、LightGBM和DIN（深度兴趣网络）多种排序策略，通过模型融合提升推荐准确性

### 技术亮点
- **Transformer生成式推荐（核心创新）**：
  - 首次将Transformer架构应用于推荐系统的召回和排序阶段，探索生成式推荐的可能性
  - 通过next-token prediction方式，将推荐问题转化为序列生成问题，能够更好地捕获用户行为序列的复杂模式
  - 一个模型同时完成召回和排序，简化推荐系统架构，实现端到端的推荐
  - 支持用户和物品特征的深度融合，通过位置编码和注意力机制学习序列模式
- **多策略融合召回**：集成5种召回策略，通过加权融合提升召回覆盖率和多样性，有效解决冷启动问题
- 实现了完整的推荐系统pipeline（召回-特征工程-排序）
- 针对大规模数据实现内存优化和高效处理流程，支持批量推理和高效检索
- **模型评估体系**：实现了召回率、NDCG等指标评估，支持离线验证和线上效果评估

### 项目成果
- **核心成果**：创新性地将Transformer应用于推荐系统，实现生成式召回和排序，探索了端到端推荐的可能性
- 实现了完整的推荐系统架构，涵盖召回、排序全流程
- Transformer模型支持用户和物品特征的深度融合，能够更好地捕获用户行为序列模式
- 通过多策略融合（5种召回策略），提升了推荐系统的覆盖率和准确性
- 构建了可扩展的特征工程框架，支持快速迭代和优化

---

## 更简洁版本（适合简历一栏）

**新闻推荐系统** | Python, PyTorch, TensorFlow, LightGBM  
• **核心创新**：将Transformer架构应用于推荐系统，实现生成式召回和排序。通过next-token prediction方式训练，一个模型同时完成召回和排序，实现端到端的推荐  
• Transformer模型支持用户ID、用户特征和物品特征的深度融合，使用自注意力机制捕获用户行为序列的长期依赖关系  
• 模型输出物品logits分数，既可以用于召回（TopK检索），也可以直接用于排序预测，简化推荐系统架构  
• 集成ItemCF、Embedding、Word2Vec、YouTubeDNN等传统方法作为补充，通过多策略融合提升召回覆盖率  
• 处理30万用户、300万点击的大规模数据，实现内存优化和高效处理流程

---

## 超简洁版本（适合简历项目列表）

**新闻推荐系统**  
核心创新：将Transformer架构应用于推荐系统，实现生成式召回和排序。通过next-token prediction方式训练，一个模型同时完成召回和排序，实现端到端的推荐。Transformer支持用户和物品特征的深度融合，使用自注意力机制捕获用户行为序列的长期依赖关系。还集成了ItemCF、Embedding、Word2Vec、YouTubeDNN等传统方法作为补充。处理30万用户、300万点击的大规模数据。

